{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.scipy.special import logsumexp\n",
    "from jax.example_libraries import optimizers\n",
    "import numpy as onp\n",
    "import jax.numpy as np\n",
    "from jax import grad, jit, vmap, value_and_grad\n",
    "from jax import random\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.PRNGKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(x):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "def relu_layer(x, w, b):\n",
    "    return ReLU(np.dot(w, x) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_mlp(sizes, key):\n",
    "    keys = random.split(key, len(sizes))\n",
    "    def initialize_layer(m, n, key, scale=1e-2):\n",
    "        w_key, b_key = random.split(key)\n",
    "        return scale * random.normal(w_key, (n, m)), scale * random.normal(b_key, (n,))\n",
    "    return [initialize_layer(m, n, k) for m, n, k in zip(sizes[:-1], sizes[1:], keys)]\n",
    "\n",
    "layer_sizes = [784, 512, 512, 10] # 28x28 img -> 10 classes\n",
    "params = initialize_mlp(layer_sizes, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_layer(x, w, b):\n",
    "    \"\"\" A single layer of a neural network \"\"\"\n",
    "    return np.maximum(0, np.dot(x, w) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(params, features):\n",
    "    activations = features\n",
    "    for w, b in params[:-1]:\n",
    "        activations = relu_layer(activations, w, b)\n",
    "    w, b = params[-1]\n",
    "    logits = np.dot(w, activations) + b\n",
    "    return logits - logsumexp(logits, axis=1, keepdims=True)\n",
    "\n",
    "batch_forward = vmap(forward_pass, in_axes=(None, 0), out_axes=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(x, k, dtype=np.float32):\n",
    "    return np.array(x[:, None] == np.arange(k), dtype)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2177f1ca12c1330a133c1d40b46100b268ab447cddcbdfdc0c7b2b7e4840e700"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('myenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
